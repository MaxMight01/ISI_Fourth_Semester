\chapter{RANDOM VECTORS}
\section{Definitions and Moments}
\textit{January 8th.}

Recall that for a random variable $X$, the $r$-th \eax{raw moment} is defined as
\begin{align}
    \mu'_r = \E[X^{r}]
\end{align}
provided the expectation exists. Here, $\mu'_{1} = \E[X] = \mu$ is the mean. The $r$-th \eax{central moment} is defined as
\begin{align}
    \mu_r = \E[(X - \mu)^{r}].
\end{align}
Similarly, for a set of sample points $x_1, x_2, \ldots, x_n$, the $r$-th \eax{sample raw moment} is defined as
\begin{align}
    m'_r = \frac{1}{n} \sum_{i=1}^n x_i^{r}
\end{align}
and the $r$-th \eax{sample central moment} is defined as
\begin{align}
    m_r = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^{r}
\end{align}
where $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$ is the sample mean. Note that $\mu_{2} = \Var(X)$ is the variance of $X$ and $\dfrac{\mu_{3}^{2}}{\mu_{2}^{3}}$ is the \eax{skewness} of $X$. There is also \eax{coefficient of variation} defined as $\dfrac{\sigma}{\mu}$ where $\sigma = \sqrt{\Var(X)}$ is the standard deviation of $X$. For a sample, it is defined as $\dfrac{s}{\bar{x}}$ where $s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^{2}}$ is the sample standard deviation. The \eax{correlation coefficient} between $x_{i}$'s and $y_{i}$'s is defined as
\begin{align}
    \dfrac{\frac{1}{n} \sum (x_{i} - \bar{x})(y_{i}-\bar{y})}{\sqrt{\frac{1}{n} \sum (x_{i} - \bar{x})^{2} \cdot \frac{1}{n} \sum (y_{i} - \bar{y})^{2}}}
\end{align}
It is also known as the product-moment correlation coefficient.


These definitions can be extended to random vectors as follows. For a random vector $\bv{X} = (X_{1}, X_{2}, \ldots, X_{p})^{T}$, the \eax{mean vector} is defined as
\begin{align}
    \bvs{\mu} = \E[\bv{X}] = \begin{pmatrix}
        \E[X_{1}] \\
        \E[X_{2}] \\
        \vdots \\
        \E[X_{p}]
    \end{pmatrix}
    = \begin{pmatrix}
        \mu_{1} \\
        \mu_{2} \\
        \vdots \\
        \mu_{p}
    \end{pmatrix}.
\end{align}
The \eax{dispersion matrix}, or covariance matrix, is defined as
\begin{align}
    \bvs{\Sigma} = \E[(\bv{X} - \bvs{\mu})(\bv{X} - \bvs{\mu})^{T}] = \begin{pmatrix}
        \Var(X_{1}) & \Cov(X_{1}, X_{2}) & \cdots & \Cov(X_{1}, X_{p}) \\
        \Cov(X_{2}, X_{1}) & \Var(X_{2}) & \cdots & \Cov(X_{2}, X_{p}) \\
        \vdots & \vdots & \ddots & \vdots \\
        \Cov(X_{p}, X_{1}) & \Cov(X_{p}, X_{2}) & \cdots & \Var(X_{p})
    \end{pmatrix} =
    \begin{pmatrix}
        \sigma_{1}^{2} & \sigma_{12} & \cdots & \sigma_{1p} \\
        \sigma_{21} & \sigma_{2}^{2} & \cdots & \sigma_{2p} \\
        \vdots & \vdots & \ddots & \vdots \\
        \sigma_{p1} & \sigma_{p2} & \cdots & \sigma_{p}^{2}
    \end{pmatrix}.
\end{align}
Similar to the correlation coefficient for random variables, we define the \eax{correlation matrix} for a random vector $\bv{X}$ as
\begin{align}
    \bvs{R} = \begin{pmatrix}
        1 & r_{12} & \cdots & r_{1p} \\
        r_{21} & 1 & \cdots & r_{2p} \\
        \vdots & \vdots & \ddots & \vdots \\
        r_{p1} & r_{p2} & \cdots & 1
    \end{pmatrix}
\end{align}
Define $D_{\sigma}$ as the diagonal matrix $\diag (\sigma_{1},\sigma_{2}, \ldots, \sigma_{p})$. Then, we have the relation
\begin{align}
    \bvs{R} = D_{\sigma}^{-1} \bvs{\Sigma} D_{\sigma}^{-1}.
\end{align}
If $\bv{Z} = \begin{pmatrix}
    \bv{X} \\ \bv{Y}
\end{pmatrix}$ is a random vector where $\bv{X}$ is of dimension $p$ and $\bv{Y}$ is of dimension $q$, then we simply have
\begin{align}
    \E[\bv{Z}] = \begin{pmatrix}
        \E[\bv{X}] \\ \E[\bv{Y}]
    \end{pmatrix}, \quad
    \Cov(\bv{Z}) = \begin{pmatrix}
        \bvs{\Sigma}_{\bv{X}} & \bvs{\Sigma}_{\bv{X}\bv{Y}} \\
        \bvs{\Sigma}_{\bv{X}\bv{Y}}^{\top} & \bvs{\Sigma}_{\bv{Y}}
    \end{pmatrix}.
\end{align}
As for linear transformations, if $\bv{Y} = \bv{A} \bv{X} + \bv{b}$ where $\bv{A}$ is a $q \times p$ matrix and $\bv{b} \in \R^{q}$, then
\begin{align}
    \E[\bv{Y}] = \bv{A} \E[\bv{X}] + \bv{b}, \quad
    \Cov(\bv{Y}) = \bv{A} \Cov(\bv{X}) \bv{A}^{\top}.
\end{align}
In particular, if $\bv{a} = \bv{A}^{\top}$ is a $p$-dimensional vector, then $Y$ is a univariate random variable written as $Y = \bv{a}^{\top} \bv{X} + b$.


\subsubsection{Mahalanobis Distance}

All of the below equations
\begin{align}
    d^{2} &= (\bv{y}_{1} - \bv{y}_{2})^{\top} \bv{S}^{-1} (\bv{y}_{1} - \bv{y}_{2}),\\
    D^{2} &= (\bar{\bv{y}} - \bvs{\mu})^{\top} \bv{S}^{-1} (\bar{\bv{y}} - \bvs{\mu}),\\
    D^{2} &= (\bar{\bv{y}} - \bvs{\mu})^{\top} \bvs{\Sigma}^{-1} (\bv{y} - \bvs{\mu}),
\end{align}
are known as the \eax{Mahalanobis distance}. Here, $S$ is the sample covariance matrix and $\bvs{\Sigma}$ is the population covariance matrix. The Mahalanobis distance is a measure of distance that accounts for the correlations between variables and the different scales of the variables. Since $\bvs{\Sigma}$ is positive semi-definite, we set $\bv{z} = \bvs{\Sigma}^{-1/2} \bv{y} - \bvs{\Sigma}^{-1/2} \bvs{\mu}$ to get
\begin{align}
    (\bv{y}-\bvs{\mu})^{\top} \bvs{\Sigma}^{-1} (\bv{y} - \bvs{\mu}) = (\bv{y}-\bvs{\mu})^{\top} \bvs{\Sigma}^{-1/2} \bvs{\Sigma}^{-1/2} (\bv{y} - \bvs{\mu}) = \bv{z}^{\top} \bv{z}.
\end{align}
This shows that the Mahalanobis distance is equivalent to the Euclidean distance in the transformed space where the data has been standardized by the covariance matrix. The covariance matrix of $\bv{z}$ is
\begin{align}
    \Cov(\bv{z}) = \bvs{\Sigma}^{-1/2} \Cov(\bv{y}) \bvs{\Sigma}^{-1/2} = \bvs{\Sigma}^{-1/2} \frac{1}{n}\bvs{\Sigma} \bvs{\Sigma}^{-1/2} = \frac{1}{n}\bv{I}_{p}.
\end{align}
Thus, the variables in $\bv{z}$ are uncorrelated with equal variances; in other words, they have been standardized. 

\subsection{Moment Generating Function}

Recall that in the univariate case, the moment generating function (MGF) of a random variable $X$ is defined as
\begin{align}
    M_{X}(t) \defeq \E[e^{tX}] = \E \left[ \sum_{r=0}^{\infty} \frac{t^r X^r}{r!} \right].
\end{align}
We extend this definition to the multivariate case as follows; for a random vector $\bv{X} = (X_{1}, X_{2}, \ldots, X_{p})^{T}$, the \eax{moment generating function} is defined as
\begin{align}
    M_{\bv{X}}(\bv{t}) \defeq \E[e^{\bv{t}^{\top} \bv{X}}] = \E \left[ e^{\sum_{i=1}^{p} t_i X_i} \right]
\end{align}
for all $\bv{t} = (t_{1}, t_{2}, \ldots, t_{p})^{T} \in \R^{p}$ such that the expectation exists. When the $X_{i}$'s are independent, we have
\begin{align}
    M_{\bv{X}}(\bv{t}) = \prod_{i=1}^{p} M_{X_{i}}(t_{i}).
\end{align}
Similar to the univariate case, the MGF can be used to compute moments of the random vector. For example, the first moment (mean vector) can be obtained by taking the gradient of the MGF at $\bv{t} = \bv{0}$:
\begin{align}
    \E[\bv{X}] = \nabla_{\bv{t}} M_{\bv{X}}(\bv{t}) \bigg|_{\bv{t}=\bv{0}}.
\end{align}
The second moment (covariance matrix) can be obtained by taking the Hessian of the MGF at $\bv{t} = \bv{0}$:
\begin{align}
    \Cov(\bv{X}) = \nabla_{\bv{t}}^{2} M_{\bv{X}}(\bv{t}) \bigg|_{\bv{t}=\bv{0}} - \E[\bv{X}] \E[\bv{X}]^{\top}.
\end{align}
Suppose $\bv{Y} = \bv{A} \bv{X} + \bv{b}$ where $\bv{A}$ is a $q \times p$ matrix and $\bv{b} \in \R^{q}$. Then, the MGF of $\bv{Y}$ is given by
\begin{align}
    M_{\bv{Y}}(\bv{t}) = e^{\bv{t}^{\top} \bv{b}} M_{\bv{X}}(\bv{A}^{\top} \bv{t}).
\end{align}

\section{Linear Models}

Generally, we had
\begin{align}
    y = \beta_{0} + \beta_{1} x + \epsilon
\end{align}
where $\epsilon \sim N(0, \sigma^{2})$. Here, we note that $y$ is a random variable while $x$ is a fixed variable. Moreover, $\sum_{i} (y_{i} - \bar{y}) = 0$ implies that $\sum_{i} \epsilon_{i} = 0$. In such a case, $\E[y] = \beta_{0} + \beta_{1} \E[x]$ and $\E[y \mid x] = \beta_{0} + \beta_{1} x$. A theorem states that if any one of the regression lines is constant, then $\Cov(X,Y) = 0$.

A simple proof is given as follows: note that $\E[Y \mid X] = k \implies \E[Y] = k$. Also,
\begin{align}
    \E[XY] = \int_{\R} \int_{\R} xy f(x,y) \, \di{x} \, \di{y} = \int_{\R} x \int_{\R} y f_{Y \mid X}(y \mid x) \, \di{y} f_{X}(x) \, \di{x} = \int_{\R} x k f_{X}(x) \, \di{x} = k \E[X].
\end{align}
Thus,
\begin{align}
    \Cov(X,Y) = \E[XY] - \E[X] \E[Y] = k \E[X] - \E[X] k = 0.
\end{align}