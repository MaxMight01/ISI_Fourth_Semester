\chapter{RANDOM VECTORS}
\section{Definitions and Moments}
\textit{January 8th.}

Recall that for a random variable $X$, the $r$-th \eax{raw moment} is defined as
\begin{align}
    \mu'_r = \E[X^{r}]
\end{align}
provided the expectation exists. Here, $\mu'_{1} = \E[X] = \mu$ is the mean. The $r$-th \eax{central moment} is defined as
\begin{align}
    \mu_r = \E[(X - \mu)^{r}].
\end{align}
Similarly, for a set of sample points $x_1, x_2, \ldots, x_n$, the $r$-th \eax{sample raw moment} is defined as
\begin{align}
    m'_r = \frac{1}{n} \sum_{i=1}^n x_i^{r}
\end{align}
and the $r$-th \eax{sample central moment} is defined as
\begin{align}
    m_r = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^{r}
\end{align}
where $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$ is the sample mean. Note that $\mu_{2} = \Var(X)$ is the variance of $X$ and $\dfrac{\mu_{3}^{2}}{\mu_{2}^{3}}$ is the \eax{skewness} of $X$. There is also \eax{coefficient of variation} defined as $\dfrac{\sigma}{\mu}$ where $\sigma = \sqrt{\Var(X)}$ is the standard deviation of $X$. For a sample, it is defined as $\dfrac{s}{\bar{x}}$ where $s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^{2}}$ is the sample standard deviation. The \eax{correlation coefficient} between $x_{i}$'s and $y_{i}$'s is defined as
\begin{align}
    \dfrac{\frac{1}{n} \sum (x_{i} - \bar{x})(y_{i}-\bar{y})}{\sqrt{\frac{1}{n} \sum (x_{i} - \bar{x})^{2} \cdot \frac{1}{n} \sum (y_{i} - \bar{y})^{2}}}
\end{align}
It is also known as the product-moment correlation coefficient.


These definitions can be extended to random vectors as follows. For a random vector $\bv{X} = (X_{1}, X_{2}, \ldots, X_{p})^{T}$, the \eax{mean vector} is defined as
\begin{align}
    \bvs{\mu} = \E[\bv{X}] = \begin{pmatrix}
        \E[X_{1}] \\
        \E[X_{2}] \\
        \vdots \\
        \E[X_{p}]
    \end{pmatrix}
    = \begin{pmatrix}
        \mu_{1} \\
        \mu_{2} \\
        \vdots \\
        \mu_{p}
    \end{pmatrix}.
\end{align}
The \eax{dispersion matrix}, or covariance matrix, is defined as
\begin{align}
    \bvs{\Sigma} = \E[(\bv{X} - \bvs{\mu})(\bv{X} - \bvs{\mu})^{T}] = \begin{pmatrix}
        \Var(X_{1}) & \Cov(X_{1}, X_{2}) & \cdots & \Cov(X_{1}, X_{p}) \\
        \Cov(X_{2}, X_{1}) & \Var(X_{2}) & \cdots & \Cov(X_{2}, X_{p}) \\
        \vdots & \vdots & \ddots & \vdots \\
        \Cov(X_{p}, X_{1}) & \Cov(X_{p}, X_{2}) & \cdots & \Var(X_{p})
    \end{pmatrix} =
    \begin{pmatrix}
        \sigma_{1}^{2} & \sigma_{12} & \cdots & \sigma_{1p} \\
        \sigma_{21} & \sigma_{2}^{2} & \cdots & \sigma_{2p} \\
        \vdots & \vdots & \ddots & \vdots \\
        \sigma_{p1} & \sigma_{p2} & \cdots & \sigma_{p}^{2}
    \end{pmatrix}.
\end{align}
Similar to the correlation coefficient for random variables, we define the \eax{correlation matrix} for a random vector $\bv{X}$ as
\begin{align}
    \bvs{R} = \begin{pmatrix}
        1 & r_{12} & \cdots & r_{1p} \\
        r_{21} & 1 & \cdots & r_{2p} \\
        \vdots & \vdots & \ddots & \vdots \\
        r_{p1} & r_{p2} & \cdots & 1
    \end{pmatrix}
\end{align}
Define $D_{\sigma}$ as the diagonal matrix $\diag (\sigma_{1},\sigma_{2}, \ldots, \sigma_{p})$. Then, we have the relation
\begin{align}
    \bvs{R} = D_{\sigma}^{-1} \bvs{\Sigma} D_{\sigma}^{-1}.
\end{align}
If $\bv{Z} = \begin{pmatrix}
    \bv{X} \\ \bv{Y}
\end{pmatrix}$ is a random vector where $\bv{X}$ is of dimension $p$ and $\bv{Y}$ is of dimension $q$, then we simply have
\begin{align}
    \E[\bv{Z}] = \begin{pmatrix}
        \E[\bv{X}] \\ \E[\bv{Y}]
    \end{pmatrix}, \quad
    \Cov(\bv{Z}) = \begin{pmatrix}
        \bvs{\Sigma}_{\bv{X}} & \bvs{\Sigma}_{\bv{X}\bv{Y}} \\
        \bvs{\Sigma}_{\bv{X}\bv{Y}}^{\top} & \bvs{\Sigma}_{\bv{Y}}
    \end{pmatrix}.
\end{align}
As for linear transformations, if $\bv{Y} = \bv{A} \bv{X} + \bv{b}$ where $\bv{A}$ is a $q \times p$ matrix and $\bv{b} \in \R^{q}$, then
\begin{align}
    \E[\bv{Y}] = \bv{A} \E[\bv{X}] + \bv{b}, \quad
    \Cov(\bv{Y}) = \bv{A} \Cov(\bv{X}) \bv{A}^{\top}.
\end{align}
In particular, if $\bv{a} = \bv{A}^{\top}$ is a $p$-dimensional vector, then $Y$ is a univariate random variable written as $Y = \bv{a}^{\top} \bv{X} + b$.


\subsubsection{Mahalanobis Distance}

All of the below equations
\begin{align}
    d^{2} &= (\bv{y}_{1} - \bv{y}_{2})^{\top} \bv{S}^{-1} (\bv{y}_{1} - \bv{y}_{2}),\\
    D^{2} &= (\bar{\bv{y}} - \bvs{\mu})^{\top} \bv{S}^{-1} (\bar{\bv{y}} - \bvs{\mu}),\\
    D^{2} &= (\bar{\bv{y}} - \bvs{\mu})^{\top} \bvs{\Sigma}^{-1} (\bv{y} - \bvs{\mu}),
\end{align}
are known as the \eax{Mahalanobis distance}. Here, $S$ is the sample covariance matrix and $\bvs{\Sigma}$ is the population covariance matrix. The Mahalanobis distance is a measure of distance that accounts for the correlations between variables and the different scales of the variables. Since $\bvs{\Sigma}$ is positive semi-definite, we set $\bv{z} = \bvs{\Sigma}^{-1/2} \bv{y} - \bvs{\Sigma}^{-1/2} \bvs{\mu}$ to get
\begin{align}
    (\bv{y}-\bvs{\mu})^{\top} \bvs{\Sigma}^{-1} (\bv{y} - \bvs{\mu}) = (\bv{y}-\bvs{\mu})^{\top} \bvs{\Sigma}^{-1/2} \bvs{\Sigma}^{-1/2} (\bv{y} - \bvs{\mu}) = \bv{z}^{\top} \bv{z}.
\end{align}
This shows that the Mahalanobis distance is equivalent to the Euclidean distance in the transformed space where the data has been standardized by the covariance matrix. The covariance matrix of $\bv{z}$ is
\begin{align}
    \Cov(\bv{z}) = \bvs{\Sigma}^{-1/2} \Cov(\bv{y}) \bvs{\Sigma}^{-1/2} = \bvs{\Sigma}^{-1/2} \frac{1}{n}\bvs{\Sigma} \bvs{\Sigma}^{-1/2} = \frac{1}{n}\bv{I}_{p}.
\end{align}
Thus, the variables in $\bv{z}$ are uncorrelated with equal variances; in other words, they have been standardized. 

\subsection{Moment Generating Function}

Recall that in the univariate case, the moment generating function (MGF) of a random variable $X$ is defined as
\begin{align}
    M_{X}(t) \defeq \E[e^{tX}] = \E \left[ \sum_{r=0}^{\infty} \frac{t^r X^r}{r!} \right].
\end{align}
We extend this definition to the multivariate case as follows; for a random vector $\bv{X} = (X_{1}, X_{2}, \ldots, X_{p})^{T}$, the \eax{moment generating function} is defined as
\begin{align}
    M_{\bv{X}}(\bv{t}) \defeq \E[e^{\bv{t}^{\top} \bv{X}}] = \E \left[ e^{\sum_{i=1}^{p} t_i X_i} \right]
\end{align}
for all $\bv{t} = (t_{1}, t_{2}, \ldots, t_{p})^{T} \in \R^{p}$ such that the expectation exists. When the $X_{i}$'s are independent, we have
\begin{align}
    M_{\bv{X}}(\bv{t}) = \prod_{i=1}^{p} M_{X_{i}}(t_{i}).
\end{align}
Similar to the univariate case, the MGF can be used to compute moments of the random vector. For example, the first moment (mean vector) can be obtained by taking the gradient of the MGF at $\bv{t} = \bv{0}$:
\begin{align}
    \E[\bv{X}] = \nabla_{\bv{t}} M_{\bv{X}}(\bv{t}) \bigg|_{\bv{t}=\bv{0}}.
\end{align}
The second moment (covariance matrix) can be obtained by taking the Hessian of the MGF at $\bv{t} = \bv{0}$:
\begin{align}
    \Cov(\bv{X}) = \nabla_{\bv{t}}^{2} M_{\bv{X}}(\bv{t}) \bigg|_{\bv{t}=\bv{0}} - \E[\bv{X}] \E[\bv{X}]^{\top}.
\end{align}
Suppose $\bv{Y} = \bv{A} \bv{X} + \bv{b}$ where $\bv{A}$ is a $q \times p$ matrix and $\bv{b} \in \R^{q}$. Then, the MGF of $\bv{Y}$ is given by
\begin{align}
    M_{\bv{Y}}(\bv{t}) = e^{\bv{t}^{\top} \bv{b}} M_{\bv{X}}(\bv{A}^{\top} \bv{t}).
\end{align}

\section{Linear Models}

Generally, we had
\begin{align}
    y = \beta_{0} + \beta_{1} x + \epsilon
\end{align}
where $\epsilon \sim N(0, \sigma^{2})$. Here, we note that $y$ is a random variable while $x$ is a fixed variable. Moreover, $\sum_{i} (y_{i} - \bar{y}) = 0$ implies that $\sum_{i} \epsilon_{i} = 0$. In such a case, $\E[y] = \beta_{0} + \beta_{1} \E[x]$ and $\E[y \mid x] = \beta_{0} + \beta_{1} x$. A theorem states that if any one of the regression lines is constant, then $\Cov(X,Y) = 0$.

A simple proof is given as follows: note that $\E[Y \mid X] = k \implies \E[Y] = k$. Also,
\begin{align}
    \E[XY] = \int_{\R} \int_{\R} xy f(x,y) \, \di{x} \, \di{y} = \int_{\R} x \int_{\R} y f_{Y \mid X}(y \mid x) \, \di{y} f_{X}(x) \, \di{x} = \int_{\R} x k f_{X}(x) \, \di{x} = k \E[X].
\end{align}
Thus,
\begin{align}
    \Cov(X,Y) = \E[XY] - \E[X] \E[Y] = k \E[X] - \E[X] k = 0.
\end{align}

\section{Multivariate Normal}
\textit{January 29th.}

Recall in the case of univariate normal distribution, we had $X \sim N(\mu, \sigma^{2})$ with probability density function (PDF)
\begin{align}
    f(x) = \begin{cases}
        \dfrac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\dfrac{(x - \mu)^{2}}{2 \sigma^{2}}\right), & x \in \R,\\[10pt]
        0, & \text{otherwise}.
    \end{cases}
\end{align}
We extend this to the multivariate case as follows. A random vector $\bv{X} = (X_{1}, X_{2}, \ldots, X_{p})^{T}$ is said to have a \eax{multivariate normal distribution} with mean vector $\bvs{\mu} \in \R^{p}$ and covariance matrix $\bvs{\Sigma}$ (denoted as $\bv{X} \sim N_{p}(\bvs{\mu}, \bvs{\Sigma})$) if its probability density function (PDF) is given by
\begin{align}
    f(\bv{x}) = \begin{cases}
        \dfrac{1}{(2 \pi)^{p/2} |\bvs{\Sigma}|^{1/2}} \exp \left( -\dfrac{1}{2} (\bv{x} - \bvs{\mu})^{\top} \bvs{\Sigma}^{-1} (\bv{x} - \bvs{\mu}) \right), & \bv{x} \in \R^{p},\\[10pt]
        0, & \text{otherwise}.
    \end{cases}
\end{align}
where $\bvs{\Sigma}$ is positive definite. The moment generating function (MGF) of $\bv{X}$ is given by
\begin{align}
    M_{\bv{X}}(\bv{t}) = \exp \left( \bv{t}^{\top} \bvs{\mu} + \dfrac{1}{2} \bv{t}^{\top} \bvs{\Sigma} \bv{t} \right).
\end{align}
For further results, we make use of the following lemmas.
\begin{lemma}
    If $\bvs{\Sigma}$ is a positive definite matrix, then there exists a non-singular matrix $\bv{B}$ such that $\bvs{\Sigma} = \bv{B}^{\top} \bv{B}$.
\end{lemma}
\begin{lemma}
    If $\bv{X} \sim N_{p}(\bvs{\mu}, \bvs{\Sigma})$ and $\bv{B}$ is as above $(\bvs{\Sigma} = \bv{B}^{\top} \bv{B})$ and satisfies $\bv{B}^{\top} \bv{Y} = \bv{X} - \bvs{\mu}$, then $\bv{Y} \sim N_{p}(\bv{0}, \bv{I}_{p})$.
\end{lemma}
\begin{proof}
    The Jacobian of the transformation is given by $|\bv{B}| = \sqrt{|\bvs{\Sigma}|}$. Also, $\bv{B} \bvs{\Sigma}^{-1} \bv{B}^{\top} = \bv{I}_{p}$. Thus,
    \begin{align}
        (\bv{X}-\bvs{\mu})^{\top} \bvs{\Sigma}^{-1} (\bv{X}-\bvs{\mu}) = \bv{Y}^{\top} \bv{Y}.
    \end{align}
    Therefore, the PDF of $\bv{Y}$ is given by
    \begin{align}
        f_{\bv{Y}}(\bv{y}) &= f_{\bv{X}}(\bv{B}^{\top} \bv{y} + \bvs{\mu}) |\bv{B}| \\
        &= \dfrac{1}{(2 \pi)^{p/2} |\bvs{\Sigma}|^{1/2}} \exp \left( -\dfrac{1}{2} \bv{y}^{\top} \bv{y} \right) |\bv{B}| \\
        &= \dfrac{1}{(2 \pi)^{p/2}} \exp \left( -\dfrac{1}{2} \bv{y}^{\top} \bv{y} \right).
    \end{align}
    This shows that $\bv{Y} \sim N_{p}(\bv{0}, \bv{I}_{p})$.
\end{proof}
\begin{lemma}
    If $X \sim (0,1)$, then $M_{X}(t) = \exp \left( \frac{t^{2}}{2} \right)$.
\end{lemma}

We can now prove the expression for the MGF of the multivariate normal distribution. We have
\begin{align}
    M_{\bv{X}}(\bv{t}) &= \E[e^{\bv{t}^{\top} \bv{X}}] = \E \left[ e^{\bv{t}^{\top} (\bv{B}^{\top} \bv{Y} + \bvs{\mu})} \right] = e^{\bv{t}^{\top} \bvs{\mu}} \E \left[ e^{(\bv{B} \bv{t})^{\top} \bv{Y}} \right] \notag \\
    &= e^{\bv{t}^{\top} \bvs{\mu}} \prod_{i=1}^{p} \E \left[ e^{(\bv{b}_{i}^{\top} \bv{t}) Y_{i}} \right] = e^{\bv{t}^{\top} \bvs{\mu}} \prod_{i=1}^{p} \exp \left( \dfrac{(\bv{b}_{i}^{\top} \bv{t})^{2}}{2} \right) \notag \\
    &= e^{\bv{t}^{\top} \bvs{\mu}} \exp \left( \dfrac{1}{2} \sum_{i=1}^{p} (\bv{b}_{i}^{\top} \bv{t})^{2} \right) = e^{\bv{t}^{\top} \bvs{\mu}} \exp \left( \dfrac{1}{2} \bv{t}^{\top} \left( \sum_{i=1}^{p} \bv{b}_{i} \bv{b}_{i}^{\top} \right) \bv{t} \right) \notag \\
    &= e^{\bv{t}^{\top} \bvs{\mu}} \exp \left( \dfrac{1}{2} \bv{t}^{\top} \bvs{\Sigma} \bv{t} \right).
\end{align}


\noindent Now suppose $z = \bv{a}^{\top} \bv{X}$ where $\bv{a} \in \R^{p}$. Then, the MGF of $z$ is given by
\begin{align}
    M_{z}(t) = \E[e^{t z}] = \E[e^{t \bv{a}^{\top} \bv{X}}] = M_{\bv{X}}(t \bv{a}) = \exp \left( t \bv{a}^{\top} \bvs{\mu} + \dfrac{t^{2}}{2} \bv{a}^{\top} \bvs{\Sigma} \bv{a} \right).
\end{align}
Thus, $z \sim N(\bv{a}^{\top} \bvs{\mu}, \bv{a}^{\top} \bvs{\Sigma} \bv{a})$. Similarly, for $\bv{Z} = \bv{A} \bv{X}$ where $\bv{A}$ is a $q \times p$ matrix, we have
\begin{align}
    M_{\bv{Z}}(\bv{t}) = \E[e^{\bv{t}^{\top} \bv{Z}}] = \E[e^{\bv{t}^{\top} \bv{A} \bv{X}}] = M_{\bv{X}}(\bv{A}^{\top} \bv{t}) = \exp \left( \bv{t}^{\top} \bv{A} \bvs{\mu} + \dfrac{1}{2} \bv{t}^{\top} \bv{A} \bvs{\Sigma} \bv{A}^{\top} \bv{t} \right).
\end{align}
So, $\bv{Z} \sim N_{q}(\bv{A} \bvs{\mu}, \bv{A} \bvs{\Sigma} \bv{A}^{\top})$.


\begin{theorem}
    $\bv{X} \sim N_{p}(\bvs{\mu}, \bvs{\Sigma})$ if and only if every linear combination of the components of $\bv{X}$ follows univariate normal distribution with suitable parameters.    
\end{theorem}
\begin{proof}
    The forward direction is already shown above. For the converse, suppose $Y = \bv{L}^{\top} \bv{X} \sim N(\bv{L}^{\top} \bvs{\mu}, \bv{L}^{\top} \bvs{\Sigma} \bv{L})$ for any $\bv{L} \in \R^{p}$. Note that the MGF of $Y$ is
    \begin{align}
        M_{Y}(t) = \exp \left( t \bv{L}^{\top} \bvs{\mu} + \dfrac{t^{2}}{2} \bv{L}^{\top} \bvs{\Sigma} \bv{L} \right).
    \end{align}
    Plugging in $t=1$, we get
    \begin{align}
        M_{Y}(1) = \exp \left( \bv{L}^{\top} \bvs{\mu} + \dfrac{1}{2} \bv{L}^{\top} \bvs{\Sigma} \bv{L} \right).
    \end{align}
    Alternatively, we have $M_{Y}(1) = \E[e^{\bv{L}^{\top} \bv{X}}] = M_{\bv{X}}(\bv{L})$. Thus, by the uniqueness property of MGFs, we must have $\bv{X} \sim N_{p}(\bvs{\mu}, \bvs{\Sigma})$.
\end{proof}

\begin{theorem}
    Suppose $\bv{X} \sim N_{p}(\bvs{\mu}, \bvs{\Sigma})$ where we partition $\bv{X}$ as
    \begin{align}
        \bv{X} = \begin{pmatrix}
            \bv{X}_{1} \\ \bv{X}_{2}
        \end{pmatrix}
    \end{align}
    where $\bv{X}_{1}$ is of dimension $r$ and $\bv{X}_{2}$ is of dimension $s = p-r$. Correspondingly, we partition $\bvs{\mu}$ and $\bvs{\Sigma}$ as
    \begin{align}
        \bvs{\mu} = \begin{pmatrix}
            \bvs{\mu}_{1} \\ \bvs{\mu}_{2}
        \end{pmatrix}, \quad
        \bvs{\Sigma} = \begin{pmatrix}
            \bvs{\Sigma}_{11} & \bvs{\Sigma}_{12} \\
            \bvs{\Sigma}_{21} & \bvs{\Sigma}_{22}
        \end{pmatrix}.
    \end{align}
    Then $\bv{X}_{1}$ and $\bv{X}_{2}$ are independent if and only if $\bvs{\Sigma}_{12} = \bv{0}$ (or equivalently, $\bvs{\Sigma}_{21} = \bv{0}$).
\end{theorem}
\begin{proof}
    The converse direction is straightforward: if $\bvs{\Sigma}_{12} = \bv{0}$, then $\abs{\bvs{\Sigma}} = \abs{\bvs{\Sigma}_{11}} \abs{\bvs{\Sigma}_{22}}$ and the PDF of $\bv{X}$ can be factored into the product of the PDFs of $\bv{X}_{1}$ and $\bv{X}_{2}$, implying independence. The forward direction is shown as follows. Suppose $\bv{X}_{1}$ and $\bv{X}_{2}$ are independent. Then, the MGF of $\bv{X}$ can be written as
    \begin{align}
        M_{\bv{X}}(\bv{t}) = M_{\bv{X}_{1}}(\bv{t}_{1}) M_{\bv{X}_{2}}(\bv{t}_{2})
    \end{align}
    where $\bv{t} = \begin{pmatrix}
        \bv{t}_{1} \\ \bv{t}_{2}
    \end{pmatrix}$. Plugging in the expressions for the MGFs, we have
    \begin{align}
        &\exp \left( \bv{t}^{\top} \bvs{\mu} + \dfrac{1}{2} \bv{t}^{\top} \bvs{\Sigma} \bv{t} \right) \notag \\
        &= \exp \left( \bv{t}_{1}^{\top} \bvs{\mu}_{1} + \dfrac{1}{2} \bv{t}_{1}^{\top} \bvs{\Sigma}_{11} \bv{t}_{1} \right) \exp \left( \bv{t}_{2}^{\top} \bvs{\mu}_{2} + \dfrac{1}{2} \bv{t}_{2}^{\top} \bvs{\Sigma}_{22} \bv{t}_{2} \right).
    \end{align}
    Equating the exponents, we get
    \begin{align}
        \bv{t}_{1}^{\top} \bvs{\mu}_{1} + \bv{t}_{2}^{\top} \bvs{\mu}_{2} + \dfrac{1}{2} \begin{pmatrix}
            \bv{t}_{1}^{\top} & \bv{t}_{2}^{\top}
        \end{pmatrix} \begin{pmatrix}
            \bvs{\Sigma}_{11} & \bvs{\Sigma}_{12} \\
            \bvs{\Sigma}_{21} & \bvs{\Sigma}_{22}
        \end{pmatrix} \begin{pmatrix}
            \bv{t}_{1} \\ \bv{t}_{2}
        \end{pmatrix} = \bv{t}_{1}^{\top} \bvs{\mu}_{1} + \dfrac{1}{2} \bv{t}_{1}^{\top} \bvs{\Sigma}_{11} \bv{t}_{1} + \bv{t}_{2}^{\top} \bvs{\mu}_{2} + \dfrac{1}{2} \bv{t}_{2}^{\top} \bvs{\Sigma}_{22} \bv{t}_{2}.
    \end{align}
    Simplifying, we obtain
    \begin{align}
        \bv{t}_{1}^{\top} \bvs{\Sigma}_{12} \bv{t}_{2} + \bv{t}_{2}^{\top} \bvs{\Sigma}_{21} \bv{t}_{1} = 0.
    \end{align}
    Since this holds for all $\bv{t}_{1}$ and $\bv{t}_{2}$, we must have $\bvs{\Sigma}_{12} = \bv{0}$ and $\bvs{\Sigma}_{21} = \bv{0}$.
\end{proof}

\noindent \textit{February 5th.}

For a multivariate normal vector $\bv{X} \sim N_{p}(\bvs{\mu}, \bvs{\Sigma})$, partition it as above. Let us make the transformation $\bv{X} \to \bv{Y}$ prescribed as $\bv{Y}^{(1)}_{r \times 1} = \bv{X}^{(1)}$ and $\bv{Y}^{(2)}_{(p-r) \times 1} = \bv{X}^{(2)}_{(p-r) \times 1} + \bv{B} \bv{X}^{(1)}_{r \times 1}$, where $\bv{B}$ is an appropriate matrix of constant values. This is simply the transformation given by
\begin{align}
    \bv{Y} = \bv{M} \bv{X}, \quad \bv{M} = \begin{pmatrix}
        \bv{I}_{r} & \bv{0} \\
        \bv{B} & \bv{I}_{p-r}
    \end{pmatrix}.
\end{align}
Then, $\bv{Y} \sim N_{p}(\bv{M} \bvs{\mu}, \bv{M} \bvs{\Sigma} \bv{M}^{\top})$. We can choose $\bv{B}$ such that $\bvs{\Sigma}_{21} + \bv{B} \bvs{\Sigma}_{11} = \bv{0}$, which gives $\bv{B} = -\bvs{\Sigma}_{21} \bvs{\Sigma}_{11}^{-1}$. With this choice of $\bv{B}$, we have
\begin{align}
    \Cov(\bv{Y}) = \begin{pmatrix}
        \bvs{\Sigma}_{11} & \bv{0} \\
        \bv{0} & \bvs{\Sigma}_{22} - \bvs{\Sigma}_{21} \bvs{\Sigma}_{11}^{-1} \bvs{\Sigma}_{12}
    \end{pmatrix}.
\end{align}
Thus, $\bv{Y}^{(1)}$ and $\bv{Y}^{(2)}$ are independent. Note that $\bv{Y}^{(2)} = \bv{X}^{(2)} - \bvs{\Sigma}_{21} \bvs{\Sigma}_{11}^{-1} \bv{X}^{(1)}$. If we define $\bv{\Sigma}_{22 \cdot 1} = \bvs{\Sigma}_{22} - \bvs{\Sigma}_{21} \bvs{\Sigma}_{11}^{-1} \bvs{\Sigma}_{12}$, then we have
\begin{align}
    \bv{Y}^{(1)} \sim N_{r}(\bvs{\mu}_{1}, \bvs{\Sigma}_{11}), \quad
    \bv{Y}^{(2)} \sim N_{p-r}(\bvs{\mu}_{2} - \bvs{\Sigma}_{21} \bvs{\Sigma}_{11}^{-1} \bvs{\mu}_{1}, \bvs{\Sigma}_{22 \cdot 1}).
\end{align}
To find the distribution of $\bv{X}^{(1)}$ given $\bv{X}^{(2)}$, 
we can use the fact that $\bv{Y}^{(1)}$ and $\bv{Y}^{(2)}$ are independent. We have
\begin{align}
    f_{\bv{X}^{(1)} \mid \bv{X}^{(2)}}(\bv{x}^{(1)} \mid \bv{x}^{(2)}) &= f_{\bv{Y}^{(1)} \mid \bv{Y}^{(2)}}(\bv{y}^{(1)} \mid \bv{y}^{(2)}) = f_{\bv{Y}^{(1)}}(\bv{y}^{(1)}) \notag\\
    &= \dfrac{1}{(2 \pi)^{r/2} |\bvs{\Sigma}_{11}|^{1/2}} \exp \left( -\dfrac{1}{2} (\bv{y}^{(1)} - \bvs{\mu}_{1})^{\top} \bvs{\Sigma}_{11}^{-1} (\bv{y}^{(1)} - \bvs{\mu}_{1}) \right)
\end{align}
which can be rewritten as
\begin{align}
    \dfrac{1}{(2 \pi)^{r/2} |\bvs{\Sigma}_{11}|^{1/2}} \exp \left( -\dfrac{1}{2} (\bv{x}^{(1)} - (\bvs{\mu}_{1} + \bvs{\Sigma}_{12} \bvs{\Sigma}_{22}^{-1} (\bv{x}^{(2)} - \bvs{\mu}_{2})))^{\top} \bvs{\Sigma}_{11}^{-1} (\bv{x}^{(1)} - (\bvs{\mu}_{1} + \bvs{\Sigma}_{12} \bvs{\Sigma}_{22}^{-1} (\bv{x}^{(2)} - \bvs{\mu}_{2}))) \right).
\end{align}
Thus, $\bv{X}^{(1)} \mid \bv{X}^{(2)} = \bv{x}^{(2)} \sim N_{r}(\bvs{\mu}_{1} + \bvs{\Sigma}_{12} \bvs{\Sigma}_{22}^{-1} (\bv{x}^{(2)} - \bvs{\mu}_{2}), \bvs{\Sigma}_{11})$. For the distribution of $\bv{X}^{(2)}$ given $\bv{X}^{(1)}$, we have
\begin{align}
    f_{\bv{X}^{(2)} \mid \bv{X}^{(1)}}(\bv{x}^{(2)} \mid \bv{x}^{(1)}) = f_{\bv{Y}^{(2)} \mid \bv{Y}^{(1)}}(\bv{y}^{(2)} \mid \bv{y}^{(1)}) = f_{\bv{Y}^{(2)}}(\bv{y}^{(2)}) \notag\\
    = \dfrac{1}{(2 \pi)^{(p-r)/2} |\bvs{\Sigma}_{22 \cdot 1}|^{1/2}} \exp \left( -\dfrac{1}{2} (\bv{y}^{(2)} - (\bvs{\mu}_{2} - \bvs{\Sigma}_{21} \bvs{\Sigma}_{11}^{-1} \bvs{\mu}_{1}))^{\top} (\bvs{\Sigma}_{22 \cdot 1})^{-1} (\bv{y}^{(2)} - (\bvs{\mu}_{2} - \bvs{\Sigma}_{21} \bvs{\Sigma}_{11}^{-1} \bvs{\mu}_{1})) \right)
\end{align}
which can be rewritten as
\begin{align}
    &\dfrac{1}{(2 \pi)^{(p-r)/2} |\bvs{\Sigma}_{22 \cdot 1}|^{1/2}} \exp \left( -\dfrac{1}{2} (\bv{x}^{(2)} - (\bvs{\mu}_{2} + \bvs{\Sigma}_{21} \bvs{\Sigma}_{11}^{-1} (\bv{x}^{(1)} - \bvs{\mu}_{1})))^{\top} (\bvs{\Sigma}_{22 \cdot 1})^{-1} (\bv{x}^{(2)} - (\bvs{\mu}_{2} + \bvs{\Sigma}_{21} \bvs{\Sigma}_{11}^{-1} (\bv{x}^{(1)} - \bvs{\mu}_{1}))) \right).
\end{align}
Thus, $\bv{X}^{(2)} \mid \bv{X}^{(1)} = \bv{x}^{(1)} \sim N_{p-r}(\bvs{\mu}_{2} + \bvs{\Sigma}_{21} \bvs{\Sigma}_{11}^{-1} (\bv{x}^{(1)} - \bvs{\mu}_{1}), \bvs{\Sigma}_{22 \cdot 1})$.

\section{Dirichlet Distribution}

Just like the multivariate normal distribution is a generalization of the univariate normal distribution, the Dirichlet distribution is a generalization of the univariate beta distribution. Recall that the PDF of the gamma distribution with parameters $\alpha$ and $\lambda$ is given by
\begin{align}
    f(x) = \begin{cases}
        \dfrac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\lambda x}, & x > 0,\\
        0, & \text{otherwise}
    \end{cases}
\end{align}
where $\Gamma(\alpha) = \int_{0}^{\infty} x^{\alpha - 1} e^{-x} \, \di{x}$ is the gamma function, and the PDF of the beta distribution with parameters $\alpha$ and $\beta$ is given by
\begin{align}
    f(x) = \begin{cases}
        \dfrac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha - 1} (1-x)^{\beta - 1}, & x \in (0,1),\\
        0, & \text{otherwise}.
    \end{cases}
\end{align}

Also recall that if $X_{1}$ and $X_{2}$ are independent random variables such that $X_{1} \sim \Gamma(\alpha, 1)$ and $X_{2} \sim \Gamma(\beta, 1)$, then $X_{1} + X_{2} \sim \Gamma(\alpha + \beta, 1)$ and $\frac{X_{1}}{X_{1} + X_{2}} \sim \operatorname{Beta}(\alpha, \beta)$. We can extend this to the multivariate case as follows. Let $\bv{Y} = (Y_{1},Y_{2},\ldots,Y_{k})$ be such that $Y_{i} > 0$ for all $i$ and $\sum_{i=1}^{k} Y_{i} = 1$. Also let $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{k}$ be positive parameters. Then, $\bv{Y}$ is said to have a \eax{Dirichlet distribution} with parameters $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{k}$ (denoted as $\bv{Y} \sim \Dir(\bvs{\alpha})$ where $\bvs{\alpha} = (\alpha_{1}, \alpha_{2}, \ldots, \alpha_{k})$) if its PDF is given by
\begin{align}
    f(y) = \begin{cases}
        \dfrac{\Gamma(\alpha_{1} + \alpha_{2} + \cdots + \alpha_{k})}{\Gamma(\alpha_{1}) \Gamma(\alpha_{2}) \cdots \Gamma(\alpha_{k})} y_{1}^{\alpha_{1} - 1} y_{2}^{\alpha_{2} - 1} \cdots y_{k}^{\alpha_{k} - 1}, & y_{i} > 0 \text{ for all } i, \sum_{i=1}^{k} y_{i} = 1,\\
        0, & \text{otherwise}.
    \end{cases}
\end{align}
If we let $X_{1}, X_{2}, \ldots, X_{k}$ be independent random variables such that $X_{i} \sim \Gamma(\alpha_{i}, 1)$ for all $i$, then the joint distribution of $X_{1}, X_{2}, \ldots, X_{k}$ is given by
\begin{align}
    f(x_{1}, x_{2}, \ldots, x_{k}) = \prod_{i=1}^{k} \dfrac{1}{\Gamma(\alpha_{i})} x_{i}^{\alpha_{i} - 1} e^{-x_{i}}, \quad x_{i} > 0 \text{ for all } i.
\end{align}
If we then let $Y_{i} = \frac{X_{i}}{\sum_{j=1}^{k} X_{j}}$ for all $i$, then $\bv{Y} = (Y_{1}, Y_{2}, \ldots, Y_{k}) \sim \Dir(\bvs{\alpha})$. The moments of the Dirichlet distribution are given by
\begin{align}
    \E[Y_{i}] &= \dfrac{\alpha_{i}}{\alpha_{0}}, \\
    \Var(Y_{i}) &= \dfrac{\alpha_{i} (\alpha_{0} - \alpha_{i})}{\alpha_{0}^{2} (\alpha_{0} + 1)}, \\
    \Cov(Y_{i}, Y_{j}) &= -\dfrac{\alpha_{i} \alpha_{j}}{\alpha_{0}^{2} (\alpha_{0} + 1)}
\end{align}
where $\alpha_{0} = \sum_{i=1}^{k} \alpha_{i}$.